{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2_question_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CbfJ2GDUNgL"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from csv import reader as rdr\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import random\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from copy import deepcopy\n",
        "from sklearn.manifold import TSNE\n",
        "warnings.filterwarnings( \"ignore\" )"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYuVvfkOCvqM"
      },
      "source": [
        "class ActivationFunctions:\n",
        "  def relu(self, X):\n",
        "    return X * (X>=0)\n",
        "\n",
        "  def relu_gradient(self, X):\n",
        "    return 1*(X>=0)\n",
        "\n",
        "  def leaky_relu(self, X):\n",
        "    return X * ((X>=0)+((X!=0)*0.01))\n",
        "    \n",
        "  def leaky_relu_gradient(self, X):\n",
        "    return 1 * ((X>=0)+((X!=0)*0.01))\n",
        "\n",
        "  def sigmoid(self, X):\n",
        "    x_calc= 1/(1+np.exp(-X))\n",
        "    return x_calc\n",
        "\n",
        "  def sigmoid_gradient(self, X):\n",
        "    sig=self.sigmoid(X)\n",
        "    x_calc=sig*(1-sig)\n",
        "    return x_calc\n",
        "\n",
        "  def linear(self, X):\n",
        "    x_calc=X\n",
        "    return x_calc\n",
        "\n",
        "  def linear_gradient(self, X):\n",
        "    x_calc=np.ones(X.shape)\n",
        "    return x_calc\n",
        "\n",
        "  def tanh(self, X):\n",
        "    x_calc=np.tanh(X)\n",
        "    return x_calc\n",
        "\n",
        "  def tanh_gradient(self, X):\n",
        "    tanh=self.tanh(X)\n",
        "    x_calc=1-tanh**2\n",
        "    return x_calc\n",
        "\n",
        "  def softmax(self, X):\n",
        "    expo = np.exp(X)\n",
        "    x_calc=expo/expo.sum(axis=1, keepdims = True)\n",
        "    return x_calc\n",
        "\n",
        "  def softmax_gradient(self, X):\n",
        "    return self.sigmoid(X)\n",
        "\n",
        "  def linear(self, X):\n",
        "      x_calc=X\n",
        "      return x_calc\n",
        "  def linear_grad(self, X):\n",
        "      x_calc=np.ones(X.shape)\n",
        "      return x_calc\n",
        "  \n",
        "  def getActivation(self,X,a_function):\n",
        "    if(a_function == 'relu'):\n",
        "      return self.relu(X)\n",
        "    elif(a_function == 'leaky_relu'):\n",
        "      return self.leaky_relu(X)\n",
        "    elif(a_function == 'sigmoid'):\n",
        "      return self.sigmoid(X)\n",
        "    elif(a_function == 'tanh'):\n",
        "      return self.tanh(X)\n",
        "    elif(a_function == 'softmax'):\n",
        "      return self.softmax(X)\n",
        "  \n",
        "  def getGradientActivation(self,X,a_function):\n",
        "    if(a_function == 'relu'):\n",
        "      return self.relu_gradient(X)\n",
        "    elif(a_function == 'leaky_relu'):\n",
        "      return self.leaky_relu_gradient(X)\n",
        "    elif(a_function == 'sigmoid'):\n",
        "      return self.sigmoid_gradient(X)\n",
        "    elif(a_function == 'tanh'):\n",
        "      return self.tanh_gradient(X)\n",
        "    elif(a_function == 'softmax'):\n",
        "      return self.softmax_gradient(X)\n",
        "\n",
        "class Neuron:\n",
        "  def zeroes_initiation(self,p_size):\n",
        "    return np.zeros(p_size)\n",
        "  def random_initiation(self,p_size):\n",
        "    return np.random.rand(p_size[0],p_size[1])*0.01\n",
        "  def normal_initiation(self,p_size):\n",
        "    return np.random.normal(0,1,size = p_size)*0.01\n",
        "\n",
        "class MultiLayerPerceptron():\n",
        "  def __init__(self, n_layers, layer_sizes, activation, learning_rate, weight_init, batch_size, num_epochs):\n",
        "      self.n_layers=n_layers\n",
        "      self.layer_sizes=layer_sizes \n",
        "      self.activation=activation \n",
        "      self.learning_rate=learning_rate \n",
        "      self.weight_init=weight_init.lower()\n",
        "      self.batch_size=batch_size\n",
        "      self.num_epochs=num_epochs\n",
        "      self.train_error = []\n",
        "      self.test_error = []\n",
        "      self.act = None\n",
        "      self.preact = None\n",
        "\n",
        "      self.activation_functions = ActivationFunctions()\n",
        "      self.neuron_intialization = Neuron()\n",
        "      \n",
        "      self.weights={}\n",
        "      self.bias={}\n",
        "\n",
        "      for i in range(self.n_layers-1):\n",
        "        if(self.weight_init == 'zero'):\n",
        "          self.weights[i] = np.array(self.neuron_intialization.zeroes_initiation((self.layer_sizes[i],self.layer_sizes[i+1])))\n",
        "        if(self.weight_init == 'random'):\n",
        "          self.weights[i] = np.array(self.neuron_intialization.random_initiation((self.layer_sizes[i],self.layer_sizes[i+1])))\n",
        "        if(self.weight_init == 'normal'):\n",
        "          self.weights[i] = np.array(self.neuron_intialization.normal_initiation((self.layer_sizes[i],self.layer_sizes[i+1])))\n",
        "        self.bias[i]=np.zeros(self.layer_sizes[i+1])\n",
        "\n",
        "  def cross_entropy_loss(self, A, y):\n",
        "    n = len(y)\n",
        "    logp = - np.log(A[np.arange(n), y.argmax(axis=1)])\n",
        "    loss = np.sum(logp)/n\n",
        "    return loss\n",
        "\n",
        "  def gradient_decent(self,derivatives,X_sample,act,layer):\n",
        "    grad=act[layer-1].T.dot(derivatives[layer])/len(X_sample)\n",
        "    self.weights[layer]=self.weights[layer]-self.learning_rate*grad\n",
        "    self.bias[layer]=self.bias[layer]-self.learning_rate*np.sum(derivatives[layer],axis=0)/len(X_sample)\n",
        "\n",
        "  def predict_x_test(self,x_test,act,preact,y_test,y_sample):\n",
        "    y_test_pred=self.predict_proba(X_test)\n",
        "    test_cost=self.cross_entropy_loss(y_test_pred,y_test)\n",
        "    train_cost = self.cross_entropy_loss(act[self.n_layers-2],y_sample)\n",
        "    self.train_error.append(train_cost)\n",
        "    self.test_error.append(test_cost)\n",
        "    self.act=act\n",
        "    self.preact=preact\n",
        "\n",
        "  def fit(self, X, y,X_test=None,y_test=None):\n",
        "\n",
        "    epoch = 0\n",
        "    while(epoch < self.num_epochs):\n",
        "      for batch in range(0,X.shape[0],self.batch_size):\n",
        "        X_sample=deepcopy(X[batch:batch+self.batch_size,:])\n",
        "        y_sample=deepcopy(y[batch:batch+self.batch_size,:])\n",
        "        input_data=deepcopy(X[batch:batch+self.batch_size,:])\n",
        "        output_data=deepcopy(y[batch:batch+self.batch_size,:])\n",
        "\n",
        "        act,preact = self.forward_propogation(input_data)\n",
        "        derivatives = self.backward_propogation(output_data,act,preact)\n",
        "\n",
        "        act[-1]=X_sample\n",
        "        \n",
        "        layer = 0\n",
        "        while(layer < self.n_layers-1):\n",
        "          self.gradient_decent(derivatives,X_sample,act,layer)\n",
        "          layer += 1\n",
        "\n",
        "      if((epoch+1)%5==0):\n",
        "        train_cost = self.cross_entropy_loss(act[self.n_layers-2],y_sample)\n",
        "        print(\"epoch\",epoch,\"\\t\",train_cost)\n",
        "      if(X_test is not None):\n",
        "        self.predict_x_test(X_test,act,preact,y_test,y_sample)\n",
        "      epoch += 1\n",
        "    return self\n",
        "\n",
        "  def forward_propogation(self,input_data):\n",
        "    preact={}\n",
        "    act={}\n",
        "    \n",
        "    layer = 0\n",
        "    while(layer < self.n_layers-2):\n",
        "      hidden_output=input_data.dot(self.weights[layer])+self.bias[layer]\n",
        "      hidden_output_A=self.activation_functions.getActivation(hidden_output,self.activation)\n",
        "\n",
        "      input_data=hidden_output_A \n",
        "      preact[layer]=hidden_output  \n",
        "      act[layer]=hidden_output_A\n",
        "      layer += 1\n",
        "\n",
        "    hidden_output=input_data.dot(self.weights[self.n_layers-2])+self.bias[self.n_layers-2]      \n",
        "    preact[self.n_layers-2]=hidden_output \n",
        "    act[self.n_layers-2]=self.activation_functions.getActivation(hidden_output,'softmax')\n",
        "    return act,preact\n",
        "\n",
        "  def backward_propogation(self,y,act,preact):\n",
        "    derivatives={}\n",
        "    y_pred=act[self.n_layers-2]\n",
        "    delta=y_pred-y\n",
        "    derivatives[self.n_layers-2]=delta\n",
        "    for layer in range(self.n_layers-3,-1,-1):\n",
        "      error=delta.dot(self.weights[layer+1].T)\n",
        "      derv=self.activation_functions.getGradientActivation(preact[layer],self.activation)\n",
        "      delta=error*derv\n",
        "      derivatives[layer]=delta\n",
        "    return derivatives\n",
        "\n",
        "  def predict_proba(self, X):\n",
        "    y,_=self.forward_propogation(X)\n",
        "    return y[self.n_layers-2]\n",
        "  \n",
        "  def predict(self, X):\n",
        "    return self.predict_proba(X).argmax(axis = 1)\n",
        "  \n",
        "  def getErrorReports(self):\n",
        "    return self.train_error,self.test_error\n",
        "    "
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5xxV56l_68B"
      },
      "source": [
        "class Evaluation:\n",
        "  # To get Confusion matrix\n",
        "  def confusionMatrix(self, actual_output, predicted_output):\n",
        "    true_positive = 0\n",
        "    true_negatives = 0\n",
        "    false_positives = 0\n",
        "    false_negative = 0\n",
        "\n",
        "    assert len(actual_output) == len(predicted_output)\n",
        "    i = 0\n",
        "    while(i < len(actual_output)):\n",
        "      if((actual_output[i] == 1) and (predicted_output[i] == 1)):\n",
        "        true_positive += 1\n",
        "      if((actual_output[i] == 1) and (predicted_output[i] == 0)):\n",
        "        true_negatives += 1\n",
        "      if((actual_output[i] == 0) and (predicted_output[i] == 1)):\n",
        "        false_positives += 1\n",
        "      if((actual_output[i] == 0) and (predicted_output[i] == 0)):\n",
        "        false_negative += 1\n",
        "      i += 1\n",
        "    \n",
        "    return true_positive,true_negatives,false_positives,false_negative\n",
        "\n",
        "  # To get accuracy score\n",
        "  def accuracyScore(self, true_positive, true_negatives, false_positives, false_negative):\n",
        "    return (true_positive + true_negatives) / (true_positive + true_negatives + false_positives + false_negative)\n",
        "  \n",
        "  def accuracy2(self,actual_output,predicted_output):\n",
        "    assert len(actual_output) == len(predicted_output)\n",
        "    correctPred = 0\n",
        "    totalPred = 0\n",
        "    for i in range(0,len(actual_output)):\n",
        "      if(int(actual_output[i]) == int(predicted_output[i])):\n",
        "        correctPred += 1\n",
        "      totalPred += 1\n",
        "    return correctPred/totalPred\n",
        "\n",
        "  # To get recall score\n",
        "  def recallScore(self, true_positive, true_negatives, false_positives, false_negative):\n",
        "    result = 0\n",
        "    try:\n",
        "      result = (true_positive) / (true_positive + false_negative)\n",
        "      return result\n",
        "    except ZeroDivisionError:\n",
        "      print(\"ZeroDivisionError\")\n",
        "\n",
        "  # To get prcision Score\n",
        "  def precisionScore(self, true_positive, true_negatives, false_positives, false_negative):\n",
        "    result = 0\n",
        "    try:\n",
        "      result = (true_positive) / (true_positive + false_positives)\n",
        "      return result\n",
        "    except ZeroDivisionError:\n",
        "      print(\"ZeroDivisionError\")\n",
        "  \n",
        "  # To get F1 score\n",
        "  def F1Score(self, true_positive, true_negatives, false_positives, false_negative):\n",
        "    result = 0\n",
        "    try:\n",
        "      result = (2 * true_positive) / ((2 * true_positive) + false_positives + false_negative)\n",
        "      return result\n",
        "    except ZeroDivisionError:\n",
        "      print(\"ZeroDivisionError\")"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPws-A3U_DVK"
      },
      "source": [
        "def plot_graph(epochs,train_error,test_error,validate_error = None):\n",
        "  plt.plot(range(epochs),train_error,label = \"Training error\")\n",
        "  plt.plot(range(epochs),test_error,label = \"Test error\")\n",
        "  if(validate_error is not None):\n",
        "    plt.plot(range(epochs),validate_error,label = \"Test error\")\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Error\")\n",
        "  plt.show()"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNoZ8dYs4zC6"
      },
      "source": [
        "def read_dataset(filename):\n",
        "\tdf=pd.read_csv(filename)\n",
        "\treturn df.iloc[:,1:],df.iloc[:,0]\n",
        "\n",
        "def dataset_split(df):\n",
        "  dataset_split = np.split(df,[int(0.7*len(df)),int(0.9*len(df))])\n",
        "  return dataset_split[0],dataset_split[1],dataset_split[2]\n",
        "\n",
        "def binary_encoding(set_np):\n",
        "  finalArr=[]\n",
        "  size = len(set_np)\n",
        "  i = 0\n",
        "  while(i<len(set_np)):\n",
        "    cur =[]\n",
        "    j = 0\n",
        "    tmpLen = len(set_np[i])\n",
        "    while(j < tmpLen):\n",
        "      if set_np[i][j]<108:\n",
        "        cur.append(0)\n",
        "      else:\n",
        "        cur.append(1)\n",
        "      j+=1\n",
        "    finalArr.append(cur)\n",
        "    i+=1\n",
        "\n",
        "  return finalArr\n",
        "\n",
        "def preprocessing(filename):\n",
        "  x_vals,y_vals = read_dataset(filename)\n",
        "  y_matrix = np.zeros((y_vals.size,y_vals.max()+1))\n",
        "  tmpArr = []\n",
        "  j = 0\n",
        "  while(j<y_vals.size):\n",
        "    tmpArr.append(j)\n",
        "    j+=1\n",
        "  y_matrix [tmpArr,y_vals] = 1\n",
        "  y_vals = y_matrix\n",
        "  return x_vals,y_vals\n",
        "\n",
        "x,y = preprocessing(\"mnist_train.csv\")\n",
        "x_tmp_train,x_tmp_test,x_tmp_validation = split_dataset(x)\n",
        "y_train,y_test,y_validation = split_dataset(y)\n",
        "\n",
        "x_train = binary_encoding(x_tmp_train.to_numpy())\n",
        "x_test = binary_encoding(x_tmp_test.to_numpy())\n",
        "x_validation = binary_encoding(x_tmp_validation.to_numpy())\n",
        "scaler = StandardScaler()\n",
        "x_train=scaler.fit_transform(x_train)\n",
        "x_test=scaler.transform(x_test)\n",
        "x_validation = scaler.transform(x_validation)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f54ER2uJ_JUS"
      },
      "source": [
        "evaluation_metric = Evaluation()\n",
        "a = MultiLayerPerceptron(6, [784, 256, 128, 64, 32, 10], 'relu', 0.08, 'normal', 3000, 150)\n",
        "a.fit(x_train, y_train, x_test, y_test)\n",
        "training_error,test_error = a.getErrorReports()\n",
        "y_predicted = a.predict(x_test)\n",
        "\n",
        "print(evaluation_metric.accuracy2(y_test.argmax(axis = 1),y_predicted))\n",
        "plot_graph(150,training_error,test_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFGUaqKgNoK"
      },
      "source": [
        "evaluation_metric = Evaluation()\n",
        "a = MultiLayerPerceptron(6, [784, 256, 128, 64, 32, 10], 'sigmoid', 0.08, 'normal', 3000, 150)\n",
        "a.fit(x_train, y_train, x_test, y_test)\n",
        "training_error,test_error = a.getErrorReports()\n",
        "y_predicted = a.predict(x_test)\n",
        "\n",
        "print(evaluation_metric.accuracy2(y_test.argmax(axis = 1),y_predicted))\n",
        "plot_graph(150,training_error,test_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2Rdyx5UAlGH"
      },
      "source": [
        "evaluation_metric = Evaluation()\n",
        "a = MultiLayerPerceptron(6, [784, 256, 128, 64, 32, 10], 'softmax', 0.08, 'normal', 3000, 150)\n",
        "a.fit(x_train, y_train, x_test, y_test)\n",
        "training_error,test_error = a.getErrorReports()\n",
        "y_predicted = a.predict(x_test)\n",
        "\n",
        "print(evaluation_metric.accuracy2(y_test.argmax(axis = 1),y_predicted))\n",
        "plot_graph(150,training_error,test_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJjtPyFmAneT"
      },
      "source": [
        "evaluation_metric = Evaluation()\n",
        "a = MultiLayerPerceptron(6, [784, 256, 128, 64, 32, 10], 'leaky_relu', 0.08, 'normal', 3000, 150)\n",
        "a.fit(x_train, y_train, x_test, y_test)\n",
        "training_error,test_error = a.getErrorReports()\n",
        "y_predicted = a.predict(x_test)\n",
        "\n",
        "print(evaluation_metric.accuracy2(y_test.argmax(axis = 1),y_predicted))\n",
        "plot_graph(150,training_error,test_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baPWHXTgArqy"
      },
      "source": [
        "evaluation_metric = Evaluation()\n",
        "a = MultiLayerPerceptron(6, [784, 256, 128, 64, 32, 10], 'tanh', 0.08, 'normal', 3000, 150)\n",
        "a.fit(x_train, y_train, x_test, y_test)\n",
        "training_error,test_error = a.getErrorReports()\n",
        "y_predicted = a.predict(x_test)\n",
        "\n",
        "print(evaluation_metric.accuracy2(y_test.argmax(axis = 1),y_predicted))\n",
        "plot_graph(150,training_error,test_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAMFdOvQLH8u"
      },
      "source": [
        "clf = MLPClassifier(solver='lbfgs', alpha=0.08, hidden_layer_sizes=(256,128,64,32), random_state=1, activation = 'logistic')\n",
        "clf.fit(x_train,y_train.argmax(axis=1))\n",
        "clf.score(x_test,y_test.argmax(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owfjuB5qLK5l"
      },
      "source": [
        "evaluation_metric = Evaluation()\n",
        "a = MultiLayerPerceptron(6, [784, 256, 128, 64, 32, 10], 'relu', 0.001, 'normal', 3000, 150)\n",
        "a.fit(x_train, y_train, x_test, y_test)\n",
        "training_error,test_error = a.getErrorReports()\n",
        "y_predicted = a.predict(x_test)\n",
        "\n",
        "print(evaluation_metric.accuracy2(y_test.argmax(axis = 1),y_predicted))\n",
        "plot_graph(150,training_error,test_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAnOlK3OLile"
      },
      "source": [
        "evaluation_metric = Evaluation()\n",
        "a = MultiLayerPerceptron(6, [784, 256, 128, 64, 32, 10], 'relu', 0.01, 'normal', 3000, 150)\n",
        "a.fit(x_train, y_train, x_test, y_test)\n",
        "training_error,test_error = a.getErrorReports()\n",
        "y_predicted = a.predict(x_test)\n",
        "\n",
        "print(evaluation_metric.accuracy2(y_test.argmax(axis = 1),y_predicted))\n",
        "plot_graph(150,training_error,test_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zciqX2__LqUS"
      },
      "source": [
        "evaluation_metric = Evaluation()\n",
        "a = MultiLayerPerceptron(6, [784, 256, 128, 64, 32, 10], 'relu', 0.1, 'normal', 3000, 150)\n",
        "a.fit(x_train, y_train, x_test, y_test)\n",
        "training_error,test_error = a.getErrorReports()\n",
        "y_predicted = a.predict(x_test)\n",
        "\n",
        "print(evaluation_metric.accuracy2(y_test.argmax(axis = 1),y_predicted))\n",
        "plot_graph(150,training_error,test_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIC0vW5ILulm"
      },
      "source": [
        "evaluation_metric = Evaluation()\n",
        "a = MultiLayerPerceptron(6, [784, 256, 128, 64, 32, 10], 'relu', 1, 'normal', 3000, 150)\n",
        "a.fit(x_train, y_train, x_test, y_test)\n",
        "training_error,test_error = a.getErrorReports()\n",
        "y_predicted = a.predict(x_test)\n",
        "\n",
        "print(evaluation_metric.accuracy2(y_test.argmax(axis = 1),y_predicted))\n",
        "plot_graph(150,training_error,test_error)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}